) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
combined <- rbind(children, parents) %>%
left_join(sentiment_afinn, by = "word") %>%
group_by(doc_id, page_number) %>%
summarise(afinn = mean(afinn, na.rm = TRUE)) %>%
ungroup()
combined
})
output$dependency_plot <- renderPlotly({
req(sentiment())
# Adjust page number by starting from 2 to be aligned with the paper's page number
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
)
ggplotly(
ggplot(sentiment_data, aes(x = page_number, y = afinn)) +
geom_line(color = "steelblue", size = 1) +
geom_point(aes(color = afinn), size = 3) +
scale_x_continuous(
breaks = seq(1,59, by = 5),
labels = seq(1, 59, by = 5)
) +
scale_color_gradient2(
low = "red", mid = "white", high = "blue", midpoint = 0,
name = "Sentiment Score"
) +
labs(
title = paste("Sentiment Trend of", input$keyword_input, "(AFINN)"),
x = "Page Number",
y = "Average Sentiment",
caption = "Source: World Bank"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
)
})
}
## Run the app
shinyApp(ui = ui_2, server = server_2)
# Adjust page number by starting from 2 to be aligned with the paper's page number
# and omit the reference pages
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
) %>% filter(doc_numeric <= 59)
## Define server
server_2 <- function(input, output, session) {
### Extract 10 keywords the most used
keywords <- parsed %>%
filter(!lemma %in% stop_words$word, !upos %in% c("PUNCT", "CCONJ")) %>%
anti_join(stop_words, by = c("lemma" = "word")) %>%
group_by(lemma) %>%
count(sort = TRUE) %>%
head(n = 10) %>%
pull(lemma)
# Populate keyword choices
updateSelectInput(inputId = "keyword_input", choices = keywords)
### Reactive filtered data based on user inputted keyword
filtered <- reactive({
req(input$keyword_input)
data <- parsed %>% filter(lemma == input$keyword_input)
validate(
need(nrow(data) > 0, "No data found for the selected keyword.")
)
data
})
### Reactive keyword-specific sentiment
sentiment <- reactive({
req(filtered())
filtered_data <- filtered() %>%
mutate(
doc_id = as.character(doc_id),
page_number = as.numeric(gsub("[^0-9]", "", doc_id))
) %>%
arrange(page_number) %>%
distinct(doc_id, .keep_all = TRUE)
sentiment_afinn <- get_sentiments("afinn") %>% rename(afinn = value)
# Extract dependency of keyword and combine altogether
children <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, head_token_id, lemma),
by = c("token_id" = "head_token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
parents <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, token_id, lemma),
by = c("head_token_id" = "token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
combined <- rbind(children, parents) %>%
left_join(sentiment_afinn, by = "word") %>%
group_by(doc_id, page_number) %>%
summarise(afinn = mean(afinn, na.rm = TRUE)) %>%
ungroup()
combined
})
output$dependency_plot <- renderPlotly({
req(sentiment())
# Adjust page number by starting from 2 to be aligned with the paper's page number
# and omit the reference pages
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
) %>% filter(doc_numeric <= 59)
ggplotly(
ggplot(sentiment_data, aes(x = page_number, y = afinn)) +
geom_line(color = "steelblue", size = 1) +
geom_point(aes(color = afinn), size = 3) +
scale_x_continuous(
breaks = seq(1,59, by = 5),
labels = seq(1, 59, by = 5)
) +
scale_color_gradient2(
low = "red", mid = "white", high = "blue", midpoint = 0,
name = "Sentiment Score"
) +
labs(
title = paste("Sentiment Trend of", input$keyword_input, "(AFINN)"),
x = "Page Number",
y = "Average Sentiment",
caption = "Source: World Bank"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
)
})
}
## Run the app
shinyApp(ui = ui_2, server = server_2)
## Define server
server_2 <- function(input, output, session) {
### Extract 10 keywords the most used
keywords <- parsed %>%
filter(!lemma %in% stop_words$word, !upos %in% c("PUNCT", "CCONJ")) %>%
anti_join(stop_words, by = c("lemma" = "word")) %>%
group_by(lemma) %>%
count(sort = TRUE) %>%
head(n = 10) %>%
pull(lemma)
# Populate keyword choices
updateSelectInput(inputId = "keyword_input", choices = keywords)
### Reactive filtered data based on user inputted keyword
filtered <- reactive({
req(input$keyword_input)
data <- parsed %>% filter(lemma == input$keyword_input)
validate(
need(nrow(data) > 0, "No data found for the selected keyword.")
)
data
})
### Reactive keyword-specific sentiment
sentiment <- reactive({
req(filtered())
filtered_data <- filtered() %>%
mutate(
doc_id = as.character(doc_id),
page_number = as.numeric(gsub("[^0-9]", "", doc_id))
) %>%
arrange(page_number) %>%
distinct(doc_id, .keep_all = TRUE)
sentiment_afinn <- get_sentiments("afinn") %>% rename(afinn = value)
# Extract dependency of keyword and combine altogether
children <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, head_token_id, lemma),
by = c("token_id" = "head_token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
parents <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, token_id, lemma),
by = c("head_token_id" = "token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
combined <- rbind(children, parents) %>%
left_join(sentiment_afinn, by = "word") %>%
group_by(doc_id, page_number) %>%
summarise(afinn = mean(afinn, na.rm = TRUE)) %>%
ungroup()
combined
})
output$dependency_plot <- renderPlotly({
req(sentiment())
# Adjust page number by starting from 2 to be aligned with the paper's page number
# and omit the reference pages
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
) %>% filter(page_number <= 59)
ggplotly(
ggplot(sentiment_data, aes(x = page_number, y = afinn)) +
geom_line(color = "steelblue", size = 1) +
geom_point(aes(color = afinn), size = 3) +
scale_x_continuous(
breaks = seq(1,59, by = 5),
labels = seq(1, 59, by = 5)
) +
scale_color_gradient2(
low = "red", mid = "white", high = "blue", midpoint = 0,
name = "Sentiment Score"
) +
labs(
title = paste("Sentiment Trend of", input$keyword_input, "(AFINN)"),
x = "Page Number",
y = "Average Sentiment",
caption = "Source: World Bank"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
)
})
}
## Run the app
shinyApp(ui = ui_2, server = server_2)
style_file("shinyapp.R")
## Define server
server_2 <- function(input, output, session) {
### Extract 10 keywords the most used
keywords <- parsed %>%
filter(!lemma %in% stop_words$word, !upos %in% c("PUNCT", "CCONJ")) %>%
anti_join(stop_words, by = c("lemma" = "word")) %>%
group_by(lemma) %>%
count(sort = TRUE) %>%
head(n = 10) %>%
pull(lemma)
# Populate keyword choices
updateSelectInput(inputId = "keyword_input", choices = keywords)
### Reactive filtered data based on user inputted keyword
filtered <- reactive({
req(input$keyword_input)
data <- parsed %>% filter(lemma == input$keyword_input)
validate(
need(nrow(data) > 0, "No data found for the selected keyword.")
)
data
})
### Reactive keyword-specific sentiment
sentiment <- reactive({
req(filtered())
filtered_data <- filtered() %>%
mutate(
doc_id = as.character(doc_id),
page_number = as.numeric(gsub("[^0-9]", "", doc_id))
) %>%
arrange(page_number) %>%
distinct(doc_id, .keep_all = TRUE)
sentiment_afinn <- get_sentiments("afinn") %>% rename(afinn = value)
# Extract dependency of keyword and combine altogether
children <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, head_token_id, lemma),
by = c("token_id" = "head_token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
parents <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, token_id, lemma),
by = c("head_token_id" = "token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
combined <- rbind(children, parents) %>%
left_join(sentiment_afinn, by = "word") %>%
group_by(doc_id, page_number) %>%
summarise(afinn = mean(afinn, na.rm = TRUE)) %>%
ungroup()
combined
})
output$dependency_plot <- renderPlotly({
req(sentiment())
# Adjust page number by starting from 2 to be aligned with the paper's page number
# and omit the reference pages
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
) %>%
filter(page_number <= 59)
ggplotly(
ggplot(sentiment_data, aes(x = page_number, y = afinn)) +
geom_line(color = "steelblue", size = 1) +
geom_point(aes(color = afinn), size = 3) +
scale_x_continuous(
breaks = seq(1, 59, by = 5),
labels = seq(1, 59, by = 5)
) +
scale_color_gradient2(
low = "red", mid = "white", high = "blue", midpoint = 0,
name = "Sentiment Score"
) +
labs(
title = paste("Sentiment Trend of", input$keyword_input, "(AFINN)"),
x = "Page Number",
y = "Average Sentiment",
caption = "Source: World Bank"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
)
})
}
## Run the app
shinyApp(ui = ui_2, server = server_2)
## Define server
server_2 <- function(input, output, session) {
### Extract 10 keywords the most used
keywords <- parsed %>%
filter(!lemma %in% stop_words$word, !upos %in% c("PUNCT", "CCONJ")) %>%
anti_join(stop_words, by = c("lemma" = "word")) %>%
group_by(lemma) %>%
count(sort = TRUE) %>%
head(n = 10) %>%
pull(lemma)
# Populate keyword choices
updateSelectInput(inputId = "keyword_input", choices = keywords)
### Reactive filtered data based on user inputted keyword
filtered <- reactive({
req(input$keyword_input)
data <- parsed %>% filter(lemma == input$keyword_input)
validate(
need(nrow(data) > 0, "No data found for the selected keyword.")
)
data
})
### Reactive keyword-specific sentiment
sentiment <- reactive({
req(filtered())
filtered_data <- filtered() %>%
mutate(
doc_id = as.character(doc_id),
page_number = as.numeric(gsub("[^0-9]", "", doc_id))
) %>%
arrange(page_number) %>%
distinct(doc_id, .keep_all = TRUE)
sentiment_afinn <- get_sentiments("afinn") %>% rename(afinn = value)
# Extract dependency of keyword and combine altogether
children <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, head_token_id, lemma),
by = c("token_id" = "head_token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
parents <- filtered_data %>%
inner_join(
parsed %>% select(doc_id, token_id, lemma),
by = c("head_token_id" = "token_id", "doc_id" = "doc_id")
) %>%
select(doc_id, page_number, lemma.y) %>%
rename(word = lemma.y)
combined <- rbind(children, parents) %>%
left_join(sentiment_afinn, by = "word") %>%
group_by(doc_id, page_number) %>%
summarise(afinn = mean(afinn, na.rm = TRUE)) %>%
ungroup()
combined
})
output$dependency_plot <- renderPlotly({
req(sentiment())
# Adjust page number by starting from 2 to be aligned with the paper's page number
# and omit the reference pages
sentiment_data <- sentiment() %>%
mutate(
page_number = page_number - min(page_number[-1]) + 1
) %>%
filter(page_number <= 59)
ggplotly(
ggplot(sentiment_data, aes(x = page_number, y = afinn)) +
geom_line(color = "steelblue", size = 1) +
geom_point(aes(color = afinn), size = 3) +
scale_x_continuous(
breaks = seq(1, 59, by = 5),
labels = seq(1, 59, by = 5)
) +
scale_color_gradient2(
low = "red", mid = "white", high = "blue", midpoint = 0,
name = "Sentiment Score"
) +
labs(
title = paste("Sentiment Trend of", input$keyword_input, "(AFINN)"),
x = "Page Number",
y = "Average Sentiment"
) +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)
)
})
}
## Run the app
shinyApp(ui = ui_2, server = server_2)
shinyApp(ui = ui, server = server)
style_file("data.R")
# Load and retrieve raw data
## Define data folder path
data_path <- "data/"
global_findex_raw <- read_xlsx(path_global_findex,
sheet = "Data"
)
library(readxl)
## Load Global Findex 2021 Database
path_global_findex <- paste0(data_path, "DatabankWide.xlsx")
style_file("staticplot.R")
style_file("model.R")
knitr::opts_chunk$set(echo = TRUE)
stargazer(
fe_model,
se = list(se_clustered),
type = "latex",
out = "model_results.latex",
dep.var.labels = "Poverty Headcount Ratio",
covariate.labels = c(
"Account Ownership (Income, poorest 40% (% ages 15+))",
"Savings (Income, poorest 40% (% ages 15+))",
"Borrowing (Income, poorest 40% (% ages 15+))",
"Digital Payments (Income, poorest 40% (% ages 15+))",
"Real GDP Growth"
),
title = "Fixed Effects Model with Clustered Standard Errors",
align = TRUE,
no.space = TRUE
)
# Create a summary table
stargazer(
fe_model,
se = list(se_clustered),
type = "text",
out = "model_results.txt",
dep.var.labels = "Poverty Headcount Ratio",
covariate.labels = c(
"Account Ownership (Income, poorest 40% (% ages 15+))",
"Savings (Income, poorest 40% (% ages 15+))",
"Borrowing (Income, poorest 40% (% ages 15+))",
"Digital Payments (Income, poorest 40% (% ages 15+))",
"Real GDP Growth"
),
title = "Fixed Effects Model with Clustered Standard Errors",
align = TRUE,
no.space = TRUE
)
library(tidyverse)
library(plm)
library(stargazer)
library(lmtest)
library(styler)
# Load the prepared dataset
data <- read_csv("data/inclusive_poverty_gdp_model.csv")
# Model fit (Fixed Effects Model)
fe_model <- plm(
poverty_headcount_ratio ~ per_account_ownership_poor + per_saving_poor +
per_borrowing_poor + per_digital_payment_poor + gdp,
data = data,
index = c("country.name", "year"),
model = "within"
)
)
# Cluster standard error
clustered_se <- vcovHC(fe_model, method = "arellano", type = "HC0", cluster = "group")
se_clustered <- sqrt(diag(clustered_se))
# Create a summary table
stargazer(
fe_model,
se = list(se_clustered),
type = "text",
out = "model_results.txt",
dep.var.labels = "Poverty Headcount Ratio",
covariate.labels = c(
"Account Ownership (Income, poorest 40% (% ages 15+))",
"Savings (Income, poorest 40% (% ages 15+))",
"Borrowing (Income, poorest 40% (% ages 15+))",
"Digital Payments (Income, poorest 40% (% ages 15+))",
"Real GDP Growth"
),
title = "Fixed Effects Model with Clustered Standard Errors",
align = TRUE,
no.space = TRUE
)
# Create a summary table
stargazer(
fe_model,
se = list(se_clustered),
type = "latex",
out = "model_results.latex",
dep.var.labels = "Poverty Headcount Ratio",
covariate.labels = c(
"Account Ownership (Income, poorest 40% (% ages 15+))",
"Savings (Income, poorest 40% (% ages 15+))",
"Borrowing (Income, poorest 40% (% ages 15+))",
"Digital Payments (Income, poorest 40% (% ages 15+))",
"Real GDP Growth"
),
title = "Fixed Effects Model with Clustered Standard Errors",
align = TRUE,
no.space = TRUE
)
